configfile: "config.yaml"

from os.path import *
import glob

# FILES = glob.glob(config["input_dir"] + "DRG_037/DRG_037-F01/*.dupes_marked.bam", recursive = True) # 1 sample
FILES = glob.glob(config["input_dir"] + "DRG_037/*/*.dupes_marked.bam", recursive = True) # 43 samples
# FILES = glob.glob(config["input_dir"] + "DRG_*/*/*.dupes_marked.bam", recursive = True) # 2568 samples
# FILES = glob.glob(config["input_dir"] + "DRG_037/**/DRG_037*.dupes_marked.bam", recursive = True) # 91 samples
# FILES = glob.glob(config["input_dir"] + "DRG_*/**/DRG_*.dupes_marked.bam", recursive = True) # 7512 samples


d = {}
for f in FILES:
	d[basename(f)[:-4]] = f

rule all:
	input:
		[
			config["dest_dir_metrics"] + s + ".multiple_metrics.alignment_summary_metrics" for s in d.keys()
		]




# rule sort_bam:
# 	input: config["input_dir"] + "{run}/{sample}/{basename}.bam"
# 	output: "{run}/{sample}/bams_pre/{basename}.sorted.bam"
# 	# shell: "samtools sort -o {output} -n {input}"
# 	shell: "java -Xmx2g -jar " + config["path_picard"] + " SortSam SO=queryname I={input} O={output}"

# rule fixmate:
# 	input: "{run}/{sample}/bams_pre/{basename}.sorted.bam"
# 	output: "{run}/{sample}/bams_pre/{basename}.fixed_mate.bam"
# 	# shell: "samtools fixmate {input} {output}"
# 	shell: "java -Xmx2g -jar " + config["path_picard"] + " FixMateInformation I={input} O={output}"


# another option to the following step is samtools collate + samtools fastq
rule picard_SamToFastq:
	input:
		lambda wildcards: d[wildcards.sample]
	benchmark:
		config["path_benchmark"] + "{sample}.SamToFastq.benchmark.txt"
	output:
		fq1 = config["dest_dir_FASTQs"] + "{sample}.R1.fq.gz",
		fq2 = config["dest_dir_FASTQs"] + "{sample}.R2.fq.gz",
		fq0 = config["dest_dir_FASTQs"] + "{sample}.unpaired.fq.gz"
	shell: "java -Xmx2g -jar " + config["path_picard"] + " SamToFastq I={input} FASTQ={output.fq1} SECOND_END_FASTQ={output.fq2} UNPAIRED_FASTQ={output.fq0}"


rule realign:
	input:
		fq1 = config["dest_dir_FASTQs"] + "{sample}.R1.fq.gz",
		fq2 = config["dest_dir_FASTQs"] + "{sample}.R2.fq.gz",
		ref = config["ref_fa"]
	benchmark:
		config["path_benchmark"] + "{sample}.bwa.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.sam"
	shell:
		"bwa mem {input.ref} {input.fq1} {input.fq2} > {output}"


rule cram:
	input:
		sam = config["dest_dir_BAMs"] + "{sample}.sam",
		ref = config["ref_fa"]
	benchmark:
		config["path_benchmark"] + "{sample}.cram.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.cram"
	shell:
		config["path_to_scramble"] + " -I sam -O cram -7 -V 3.0 -t 32 -B -r {input.ref} {input.sam} {output}"


rule flagstats:
	input:
		sam = config["dest_dir_BAMs"] + "{sample}.sam",
		cram = config["dest_dir_BAMs"] + "{sample}.cram",
		original_bam = lambda wildcards: d[wildcards.sample]
	benchmark:
		config["path_benchmark"] + "{sample}.flagstat.benchmark.txt"
	output:
		flagstat_sam = config["dest_dir_metrics"] + "{sample}.flagstat_sam",
		flagstat_cram = config["dest_dir_metrics"] + "{sample}.flagstat_cram",
		flagstat_original_bam = config["dest_dir_metrics"] + "{sample}.flagstat_original_bam"
	shell:
		"samtools flagstat {input.sam} > {output.flagstat_sam} && "
		"samtools flagstat {input.cram} > {output.flagstat_cram} && "
		"samtools flagstat {input.original_bam} > {output.flagstat_original_bam} "


rule sort_bam:
	input:
		sam = config["dest_dir_BAMs"] + "{sample}.sam",
		flagstat = config["dest_dir_metrics"] + "{sample}.flagstat_original_bam"
	benchmark:
		config["path_benchmark"] + "{sample}.sort_sam.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.sorted.bam"
	shell:
		"java -Xmx2g -jar " + config["path_picard"] + " SortSam SO=coordinate I={input.sam} O={output}"


rule mark_duplicates:
	input:
		config["dest_dir_BAMs"] + "{sample}.sorted.bam"
	benchmark:
		config["path_benchmark"] + "{sample}.mark_duplicates.benchmark.txt"
	output:
		bam = config["dest_dir_BAMs"] + "{sample}.md.bam",
		metrics = config["dest_dir_metrics"] + "{sample}.duplicate_metrics.txt"
	shell:
		"java -Xmx2g -jar " + config["path_picard"] + " MarkDuplicates "
		"I={input} "
		"O={output.bam} "
		"M={output.metrics} "
		"VALIDATION_STRINGENCY=SILENT REMOVE_DUPLICATES=false"

rule add_RG:
	input:
		original_bam = lambda wildcards: d[wildcards.sample],
		bam = config["dest_dir_BAMs"] + "{sample}.md.bam"
	benchmark:
		config["path_benchmark"] + "{sample}.AddOrReplaceReadGroups.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.with_RG.bam"
	shell:
		"extra=$(samtools view -H {input.original_bam} | grep '^@RG' | sed -e 's/@RG//g' | sed -e 's/\\t/ RG/g' | sed -e 's/:/=/g') && "
		"java -Xmx12g -jar " + config["path_picard"] + " AddOrReplaceReadGroups "
		"I={input.bam} "
		"O={output} "
		"$extra"


rule cleanSam:
	input:
		bam = config["dest_dir_BAMs"] + "{sample}.with_RG.bam",
		ref = config["ref_fa"]
	benchmark:
		config["path_benchmark"] + "{sample}.CleanSam.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.clean.bam"
	shell:
		"java -Xmx2g -jar " + config["path_picard"] + " CleanSam "
		"I={input.bam} "
		"O={output} "
		"R={input.ref}"


rule bam_index:
	input:
		config["dest_dir_BAMs"] + "{sample}.clean.bam"
	benchmark:
		config["path_benchmark"] + "{sample}.bam_index.benchmark.txt"
	output:
		config["dest_dir_BAMs"] + "{sample}.clean.bam.bai"
	shell:
		"samtools index {input}"


# rule ValidateSamFile:
# 	input:
# 		bam = config["dest_dir_BAMs"] + "{sample}.clean.bam",
# 		ref = config["ref_fa"]
# 	log:
# 		config["path_logs"] + "{sample}.ValidateSamFile.log"
# 	benchmark:
# 		config["path_benchmark"] + "{sample}.ValidateSamFile.benchmark.txt"
# 	output: config["dest_dir_metrics"] + "{sample}.validate_output"
# 	shell:
# 		"java -Xmx2g -jar " + config["path_picard"] + " ValidateSamFile "
# 		"I={input.bam} O={output} REFERENCE_SEQUENCE={input.ref}"

# MODE=SUMMARY REFERENCE_SEQUENCE={input.ref} QUIET=true VALIDATION_STRINGENCY=SILENT VERBOSITY=null IGNORE_WARNINGS=true"


# collect multiple metrics, verifyBamID...
rule verifyBamID:
	input:
		bam = config["dest_dir_BAMs"] + "{sample}.clean.bam",
		vcf = config["vcf_ref_verifyBamID"],
		index = config["dest_dir_BAMs"] + "{sample}.clean.bam.bai"
	benchmark:
		config["path_benchmark"] + "{sample}.verifyBamID.benchmark.txt"
	output:
		config["dest_dir_metrics"] + "{sample}.verifyBamID.selfSM"
	shell:
		"verifyBamID "
		"--vcf {input.vcf} "
		"--bam {input.bam} "
		"--out " + config["dest_dir_metrics"] + "{wildcards.sample}.verifyBamID "
		"--ignoreRG --verbose"



# IMPORTANT!! in WES or targeted sequencing we must use Picard CollectHsMetrics instead
# rule CollectHsMetrics:
# 	input:
# 		bam = config["dest_dir_BAMs"] + "{sample}.clean.bam",
# 		ref = config["ref_fa"],
# 		bait_intervals = config["path_bait_intervals"],
# 		target_intervals = config["path_target_intervals"]
# 	benchmark:
# 		config["path_benchmark"] + "{sample}.CollectHsMetrics.benchmark.txt"
# 	output: config["dest_dir_metrics"] + "{sample}.hs_metrics.txt"
# 	shell:
# 		"java -Xmx12g -jar " + config["path_picard"] + " CollectHsMetrics "
# 		"R={input.ref} I={input.bam} O={output} "
# 		"BAIT_INTERVALS={input.bait_intervals} TARGET_INTERVALS={input.target_intervals} "


# rule CollectWgsMetrics:
# 	input:
# 		bam = config["dest_dir_BAMs"] + "{sample}.clean.bam",
# 		ref = config["ref_fa"]
# 	benchmark:
# 		config["path_benchmark"] + "{sample}.CollectWgsMetrics.benchmark.txt"
# 	output: config["dest_dir_metrics"] + "{sample}.wgs_metrics.txt"
# 	shell:
# 		"java -Xmx12g -jar " + config["path_picard"] + " CollectWgsMetrics "
# 		"R={input.ref} I={input.bam} O={output}"



rule CollectMultipleMetrics:
	input:
		bam = config["dest_dir_BAMs"] + "{sample}.clean.bam",
		ref = config["ref_fa"],
		verifyBamID = config["dest_dir_metrics"] + "{sample}.verifyBamID.selfSM"
	benchmark:
		config["path_benchmark"] + "{sample}.CollectMultipleMetrics.benchmark.txt"
	output:
		config["dest_dir_metrics"] + "{sample}.multiple_metrics.alignment_summary_metrics"
	shell:
		"java -Xmx12g -jar " + config["path_picard"] + " CollectMultipleMetrics "
		"R={input.ref} I={input.bam} O=" + config["dest_dir_metrics"] + "{wildcards.sample}.multiple_metrics "
		"PROGRAM=null "
		"PROGRAM=CollectAlignmentSummaryMetrics "
		"PROGRAM=CollectInsertSizeMetrics "
		"PROGRAM=CollectGcBiasMetrics "
		"METRIC_ACCUMULATION_LEVEL=null "
		"METRIC_ACCUMULATION_LEVEL=READ_GROUP "
		"METRIC_ACCUMULATION_LEVEL=SAMPLE "
		"VALIDATION_STRINGENCY=SILENT "



# compare the flagstats of the original bam vs new bam
# check the metrics and add the sample to a exclude.txt file
# open fastqs with FastQC > possibly remove adapters with cutadapt.

# snakemake -s part1.snake -np --cluster bsub --jobs 10



